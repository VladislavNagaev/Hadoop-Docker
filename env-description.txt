# core-site.xml

fs.defaultFS
$CORE_CONF_fs_defaultFS
https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-common/core-default.xml#fs.defaultFS
The name of the default file system. A URI whose scheme and authority determine the FileSystem implementation. 
The uri's scheme determines the config property (fs.SCHEME.impl) naming the FileSystem implementation class. 
The uri's authority is used to determine the host, port, etc. for a filesystem.

hadoop.http.staticuser.user
$CORE_CONF_hadoop_http_staticuser_user
https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-common/core-default.xml#hadoop.http.staticuser.user
The user name to filter as, on static web filters while rendering content. 
An example use is the HDFS web UI (user to be used for browsing files).

hadoop.proxyuser.hue.hosts
$CORE_CONF_hadoop_proxyuser_hue_hosts
https://hadoop.apache.org/docs//r3.3.4/hadoop-project-dist/hadoop-common/Superusers.html

hadoop.proxyuser.hue.groups
$CORE_CONF_hadoop_proxyuser_hue_groups
https://hadoop.apache.org/docs//r3.3.4/hadoop-project-dist/hadoop-common/Superusers.html

io.compression.codecs
$CORE_CONF_io_compression_codecs
https://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/io/compress/SnappyCodec.html
https://hadoop.apache.org/docs/r3.3.4/api/org/apache/hadoop/io/compress/package-summary.html

hadoop.tmp.dir
$CORE_CONF_tmp_dir
https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-common/core-default.xml#hadoop.tmp.dir
A base for other temporary directories.



# hdfs-site.xml

dfs.replication
$HDFS_CONF_dfs_replication
https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.replication
Default block replication. The actual number of replications can be specified when the file is created. 
The default is used if replication is not specified in create time.

dfs.blocksize
$HDFS_CONF_dfs_blocksize
https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.blocksize
The default block size for new files, in bytes. You can use the following suffix (case insensitive): 
k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.), 
Or provide complete size in bytes (such as 134217728 for 128 MB).

dfs.permissions.enabled
$HDFS_CONF_dfs_permissions_enabled
https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html
If yes use the permissions system as described here. If no, permission checking is turned off, but all other behavior is unchanged. 
Switching from one parameter value to the other does not change the mode, owner or group of files or directories. 
Regardless of whether permissions are on or off, chmod, chgrp, chown and setfacl always check permissions. 
These functions are only useful in the permissions context, and so there is no backwards compatibility issue. 
Furthermore, this allows administrators to reliably set owners and permissions in advance of turning on regular permissions checking.

dfs.namenode.datanode.registration.ip-hostname-check
$HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check
https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.namenode.datanode.registration.ip-hostname-check
If true (the default), then the namenode requires that a connecting datanode's address must be resolved to a hostname. 
If necessary, a reverse DNS lookup is performed. All attempts to register a datanode from an unresolvable address are rejected. 
It is recommended that this setting be left on to prevent accidental registration of datanodes listed by hostname in the excludes file during a DNS outage. 
Only set this to false in environments where there is no infrastructure to support reverse DNS lookup.

dfs.namenode.rpc-bind-host
$HDFS_CONF_dfs_namenode_rpc___bind___host
https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.namenode.rpc-bind-host
The actual address the RPC server will bind to. 
If this optional address is set, it overrides only the hostname portion of dfs.namenode.rpc-address. 
It can also be specified per name node or name service for HA/Federation. 
This is useful for making the name node listen on all interfaces by setting it to 0.0.0.0.

dfs.namenode.servicerpc-bind-host
$HDFS_CONF_dfs_namenode_servicerpc___bind___host
https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.namenode.servicerpc-bind-host
The actual address the service RPC server will bind to. 
If this optional address is set, it overrides only the hostname portion of dfs.namenode.servicerpc-address. 
It can also be specified per name node or name service for HA/Federation. 
This is useful for making the name node listen on all interfaces by setting it to 0.0.0.0.

dfs.namenode.http-bind-host
$HDFS_CONF_dfs_namenode_http___bind___host
https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.namenode.http-bind-host
The actual address the HTTP server will bind to. 
If this optional address is set, it overrides only the hostname portion of dfs.namenode.http-address. 
It can also be specified per name node or name service for HA/Federation. 
This is useful for making the name node HTTP server listen on all interfaces by setting it to 0.0.0.0.

dfs.namenode.https-bind-host
$HDFS_CONF_dfs_namenode_https___bind___host
https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.namenode.https-bind-host
The actual address the HTTPS server will bind to. 
If this optional address is set, it overrides only the hostname portion of dfs.namenode.https-address. 
It can also be specified per name node or name service for HA/Federation. 
This is useful for making the name node HTTPS server listen on all interfaces by setting it to 0.0.0.0.

dfs.client.use.datanode.hostname
$HDFS_CONF_dfs_client_use_datanode_hostname
https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.client.use.datanode.hostname
Whether clients should use datanode hostnames when connecting to datanodes.

dfs.datanode.use.datanode.hostname
$HDFS_CONF_dfs_datanode_use_datanode_hostname
https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.datanode.use.datanode.hostname
Whether datanodes should use datanode hostnames when connecting to other datanodes for data transfer.

dfs.namenode.name.dir
$HDFS_CONF_dfs_namenode_name_dir
https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.namenode.name.dir
Determines where on the local filesystem the DFS name node should store the name table(fsimage). 
If this is a comma-delimited list of directories then the name table is replicated in all of the directories, for redundancy.

dfs.datanode.data.dir
$HDFS_CONF_dfs_datanode_data_dir
https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.datanode.data.dir
Determines where on the local filesystem an DFS data node should store its blocks. 
If this is a comma-delimited list of directories, then data will be stored in all named directories, typically on different devices. 
The directories should be tagged with corresponding storage types ([SSD]/[DISK]/[ARCHIVE]/[RAM_DISK]) for HDFS storage policies. 
The default storage type will be DISK if the directory does not have a storage type tagged explicitly. 
Directories that do not exist will be created if local filesystem permission allows.

dfs.namenode.safemode.extension
$HDFS_CONF_dfs_namenode_safemode_extension
https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.namenode.safemode.extension
Determines extension of safe mode in milliseconds after the threshold level is reached. 
Support multiple time unit suffix (case insensitive), as described in dfs.heartbeat.interval.



# yarn-site.xml

yarn.log-aggregation-enable
$YARN_CONF_yarn_log___aggregation___enable
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.log-aggregation-enable
Whether to enable log aggregation. 
Log aggregation collects each container's logs and moves these logs onto a file-system, for e.g. HDFS, after the application completes. 
Users can configure the "yarn.nodemanager.remote-app-log-dir" and "yarn.nodemanager.remote-app-log-dir-suffix" properties to determine where these logs are moved to. 
Users can access the logs via the Application Timeline Server.

yarn.log.server.url
$YARN_CONF_yarn_log_server_url
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.log.server.url
URL for log aggregation server

yarn.resourcemanager.recovery.enabled
$YARN_CONF_yarn_resourcemanager_recovery_enabled
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.resourcemanager.recovery.enabled
Enable RM to recover state after starting. If true, then yarn.resourcemanager.store.class must be specified.

yarn.resourcemanager.store.class
$YARN_CONF_yarn_resourcemanager_store_class
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.resourcemanager.store.class
The class to use as the persistent store. 
If org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore is used, 
the store is implicitly fenced; meaning a single ResourceManager is able to use the store at any point in time. 
More details on this implicit fencing, along with setting up appropriate ACLs is discussed under yarn.resourcemanager.zk-state-store.root-node.acl.

yarn.resourcemanager.scheduler.class
$YARN_CONF_yarn_resourcemanager_scheduler_class
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.resourcemanager.scheduler.class
The class to use as the resource scheduler.

yarn.scheduler.capacity.root.default.maximum-allocation-mb
$YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___mb
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html
The per queue maximum limit of memory to allocate to each container request at the Resource Manager. 
This setting overrides the cluster configuration yarn.scheduler.maximum-allocation-mb. 
This value must be smaller than or equal to the cluster maximum.

yarn.scheduler.capacity.root.default.maximum-allocation-vcores
$YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___vcores
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html
The per queue maximum limit of virtual cores to allocate to each container request at the Resource Manager. 
This setting overrides the cluster configuration yarn.scheduler.maximum-allocation-vcores. 
This value must be smaller than or equal to the cluster maximum.

yarn.resourcemanager.fs.state-store.uri
$YARN_CONF_yarn_resourcemanager_fs_state___store_uri
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.resourcemanager.fs.state-store.uri
URI pointing to the location of the FileSystem path where RM state will be stored. 
This must be supplied when using org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore as the value for yarn.resourcemanager.store.class

yarn.system-metrics-publisher.enabled
$YARN_CONF_yarn_resourcemanager_system___metrics___publisher_enabled
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.system-metrics-publisher.enabled
The setting that controls whether yarn system metrics is published on the Timeline service or not by RM And NM.

yarn.resourcemanager.hostname
$YARN_CONF_yarn_resourcemanager_hostname
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.resourcemanager.hostname
The hostname of the RM.

yarn.resourcemanager.address
$YARN_CONF_yarn_resourcemanager_address
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.resourcemanager.address
The address of the applications manager interface in the RM.

yarn.resourcemanager.scheduler.address
$YARN_CONF_yarn_resourcemanager_scheduler_address
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.resourcemanager.scheduler.address
The address of the scheduler interface.

yarn.resourcemanager.resource-tracker.address
$YARN_CONF_yarn_resourcemanager_resource__tracker_address
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.resourcemanager.resource-tracker.address

yarn.timeline-service.enabled
$YARN_CONF_yarn_timeline___service_enabled
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.timeline-service.enabled
In the server side it indicates whether timeline service is enabled or not. 
And in the client side, users can enable it to indicate whether client wants to use timeline service. 
If its enabled in the client side along with security, then yarn client tries to fetch the delegation tokens for the timeline server.

yarn.timeline-service.generic-application-history.enabled
$YARN_CONF_yarn_timeline___service_generic___application___history_enabled
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-site/TimelineServer.html
Indicate to clients whether to query generic application data from timeline history-service or not. 
If not enabled then application data is queried only from Resource Manager. Defaults to false.

yarn.timeline-service.hostname
$YARN_CONF_yarn_timeline___service_hostname
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.timeline-service.hostname
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-site/TimelineServer.html
The hostname of the timeline service web application.

mapreduce.map.output.compress
$YARN_CONF_mapreduce_map_output_compress
https://hadoop.apache.org/docs/r3.3.4/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml#mapreduce.map.output.compress
Should the outputs of the maps be compressed before being sent across the network. Uses SequenceFile compression.

mapred.map.output.compress.codec
$YARN_CONF_mapred_map_output_compress_codec
https://hadoop.apache.org/docs/r3.3.4/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml#mapreduce.map.output.compress.codec
If the map outputs are compressed, how should they be compressed?

yarn.nodemanager.resource.detect-hardware-capabilities
$YARN_CONF_yarn_nodemanager_resource_detect___hardware___capabilities
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.nodemanager.resource.detect-hardware-capabilities
Enable auto-detection of node capabilities such as memory and CPU.

yarn.nodemanager.resource.memory-mb
$YARN_CONF_yarn_nodemanager_resource_memory___mb
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.nodemanager.resource.memory-mb
Amount of physical memory, in MB, that can be allocated for containers. 
If set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, it is automatically calculated(in case of Windows and Linux). 
In other cases, the default is 8192MB.

yarn.nodemanager.resource.cpu-vcores
$YARN_CONF_yarn_nodemanager_resource_cpu___vcores
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.nodemanager.resource.cpu-vcores
Number of vcores that can be allocated for containers. 
This is used by the RM scheduler when allocating resources for containers. 
This is not used to limit the number of CPUs used by YARN containers. 
If it is set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, 
it is automatically determined from the hardware in case of Windows and Linux. 
In other cases, number of vcores is 8 by default.

yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage
$YARN_CONF_yarn_nodemanager_disk___health___checker_max___disk___utilization___per___disk___percentage
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage
The maximum percentage of disk space utilization allowed after which a disk is marked as bad. 
Values can range from 0.0 to 100.0. If the value is greater than or equal to 100, the nodemanager will check for full disk. 
This applies to yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs when yarn.nodemanager.disk-health-checker.disk-utilization-threshold.enabled is true.

yarn.nodemanager.remote-app-log-dir
$YARN_CONF_yarn_nodemanager_remote___app___log___dir
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.nodemanager.remote-app-log-dir
Where to aggregate logs to.

yarn.nodemanager.aux-services
$YARN_CONF_yarn_nodemanager_aux___services
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.nodemanager.aux-services
A comma separated list of services where service name should only contain a-zA-Z0-9_ and can not start with numbers

yarn.resourcemanager.bind-host
$YARN_CONF_yarn_resourcemanager_bind___host
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.resourcemanager.bind-host
The actual address the server will bind to. 
If this optional address is set, the RPC and webapp servers will bind to this address and 
the port specified in yarn.resourcemanager.address and yarn.resourcemanager.webapp.address, respectively. 
This is most useful for making RM listen to all interfaces by setting to 0.0.0.0.

yarn.nodemanager.bind-host
$YARN_CONF_yarn_nodemanager_bind___host
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.nodemanager.bind-host
The actual address the server will bind to. 
If this optional address is set, the RPC and webapp servers will bind to this address and 
the port specified in yarn.nodemanager.address and yarn.nodemanager.webapp.address, respectively. 
This is most useful for making NM listen to all interfaces by setting to 0.0.0.0.

yarn.timeline-service.bind-host
$YARN_CONF_yarn_timeline___service_bind___host
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.timeline-service.bind-host
The actual address the server will bind to. 
If this optional address is set, the RPC and webapp servers will bind to this address and 
the port specified in yarn.timeline-service.address and yarn.timeline-service.webapp.address, respectively. 
This is most useful for making the service listen to all interfaces by setting to 0.0.0.0.

yarn.timeline-service.leveldb-timeline-store.path
$YARN_CONF_yarn_timeline___service_leveldb___timeline___store_path
https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/TimelineServer.html
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.timeline-service.leveldb-timeline-store.path
Store file name for leveldb timeline store. Defaults to ${hadoop.tmp.dir}/yarn/timeline.

yarn.nodemanager.linux-container-executor.group
$YARN_CONF_yarn_nodemanager_linux___container___executor_group
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-site/NodeManagerCgroups.html
The Unix group of the NodeManager. It should match the setting in “container-executor.cfg”. 
This configuration is required for validating the secure access of the container-executor binary.



# mapred-site.xml

mapreduce.framework.name
$MAPRED_CONF_mapreduce_framework_name
https://hadoop.apache.org/docs/r3.3.4/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml#mapreduce.framework.name
The runtime framework for executing MapReduce jobs. Can be one of local, classic or yarn.

mapred.child.java.opts
$MAPRED_CONF_mapred_child_java_opts
https://hadoop.apache.org/docs/r3.3.4/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml#mapred.child.java.opts
Java opts for the task processes. The following symbol, if present, will be interpolated: @taskid@ is replaced by current TaskID. Any other occurrences of '@' will go unchanged. 
For example, to enable verbose gc logging to a file named for the taskid in /tmp and to set the heap maximum to be a gigabyte, 
pass a 'value' of: -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc Usage of -Djava.library.path can cause programs to no longer function if hadoop native libraries are used. 
These values should instead be set as part of LD_LIBRARY_PATH in the map / reduce JVM env using the mapreduce.map.env and mapreduce.reduce.env config settings. 
If -Xmx is not set, it is inferred from mapreduce.{map|reduce}.memory.mb and mapreduce.job.heap.memory-mb.ratio.

mapreduce.map.memory.mb
$MAPRED_CONF_mapreduce_map_memory_mb
https://hadoop.apache.org/docs/r3.3.4/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml#mapreduce.map.memory.mb
The amount of memory to request from the scheduler for each map task. 
If this is not specified or is non-positive, it is inferred from mapreduce.map.java.opts and mapreduce.job.heap.memory-mb.ratio. 
If java-opts are also not specified, we set it to 1024

mapreduce.reduce.memory.mb
$MAPRED_CONF_mapreduce_reduce_memory_mb
https://hadoop.apache.org/docs/r3.3.4/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml#mapreduce.reduce.memory.mb
The amount of memory to request from the scheduler for each reduce task. 
If this is not specified or is non-positive, it is inferred from mapreduce.reduce.java.opts and mapreduce.job.heap.memory-mb.ratio. 
If java-opts are also not specified, we set it to 1024.

mapreduce.map.java.opts
$MAPRED_CONF_mapreduce_map_java_opts

mapreduce.reduce.java.opts
$MAPRED_CONF_mapreduce_reduce_java_opts

yarn.app.mapreduce.am.env
$MAPRED_CONF_yarn_app_mapreduce_am_env
https://hadoop.apache.org/docs/r3.3.4/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml#yarn.app.mapreduce.am.env
User added environment variables for the MR App Master processes, specified as a comma separated list. Example : 
1) A=foo This will set the env variable A to foo 
2) B=$B:c This is inherit tasktracker's B env variable. 
To define environment variables individually, you can specify multiple properties of the form yarn.app.mapreduce.am.env.VARNAME, where VARNAME is the name of the environment variable. 
This is the only way to add a variable when its value contains commas.

mapreduce.map.env
$MAPRED_CONF_mapreduce_map_env

mapreduce.reduce.env
$MAPRED_CONF_mapreduce_reduce_env

yarn.nodemanager.bind-host
$MAPRED_CONF_yarn_nodemanager_bind___host
https://hadoop.apache.org/docs/r3.3.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml#yarn.nodemanager.bind-host
The actual address the server will bind to. 
If this optional address is set, the RPC and webapp servers will bind to this address and 
the port specified in yarn.nodemanager.address and yarn.nodemanager.webapp.address, respectively. 
This is most useful for making NM listen to all interfaces by setting to 0.0.0.0.

