<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>


<configuration>

    <property>
        <name>dfs.replication</name>
        <value>$HDFS_CONF_dfs_replication</value>
        <description>
            https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.replication
            Default block replication. The actual number of replications can be specified when the file is created. 
            The default is used if replication is not specified in create time.
        </description>
    </property>

    <property>
        <name>dfs.blocksize</name>
        <value>$HDFS_CONF_dfs_blocksize</value>
        <description>
            https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.blocksize
            The default block size for new files, in bytes. You can use the following suffix (case insensitive): 
            k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.), 
            Or provide complete size in bytes (such as 134217728 for 128 MB).
        </description>
    </property>

    <property>
        <name>dfs.permissions.enabled</name>
        <value>$HDFS_CONF_dfs_permissions_enabled</value>
        <description>
            https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html
            If yes use the permissions system as described here. If no, permission checking is turned off, but all other behavior is unchanged. 
            Switching from one parameter value to the other does not change the mode, owner or group of files or directories. 
            Regardless of whether permissions are on or off, chmod, chgrp, chown and setfacl always check permissions. 
            These functions are only useful in the permissions context, and so there is no backwards compatibility issue. 
            Furthermore, this allows administrators to reliably set owners and permissions in advance of turning on regular permissions checking.
        </description>
    </property>

    <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
        <value>$HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check</value>
        <description>
            https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.namenode.datanode.registration.ip-hostname-check
            If true (the default), then the namenode requires that a connecting datanode's address must be resolved to a hostname. 
            If necessary, a reverse DNS lookup is performed. All attempts to register a datanode from an unresolvable address are rejected. 
            It is recommended that this setting be left on to prevent accidental registration of datanodes listed by hostname in the excludes file during a DNS outage. 
            Only set this to false in environments where there is no infrastructure to support reverse DNS lookup.
        </description>
    </property>

    <property>
        <name>dfs.namenode.rpc-bind-host</name>
        <value>$HDFS_CONF_dfs_namenode_rpc___bind___host</value>
        <description>
            https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.namenode.rpc-bind-host
            The actual address the RPC server will bind to. 
            If this optional address is set, it overrides only the hostname portion of dfs.namenode.rpc-address. 
            It can also be specified per name node or name service for HA/Federation. 
            This is useful for making the name node listen on all interfaces by setting it to 0.0.0.0.
        </description>
    </property>

    <property>
        <name>dfs.namenode.servicerpc-bind-host</name>
        <value>$HDFS_CONF_dfs_namenode_servicerpc___bind___host</value>
        <description>
            https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.namenode.servicerpc-bind-host
            The actual address the service RPC server will bind to. 
            If this optional address is set, it overrides only the hostname portion of dfs.namenode.servicerpc-address. 
            It can also be specified per name node or name service for HA/Federation. 
            This is useful for making the name node listen on all interfaces by setting it to 0.0.0.0.
        </description>
    </property>

    <property>
        <name>dfs.namenode.http-bind-host</name>
        <value>$HDFS_CONF_dfs_namenode_http___bind___host</value>
        <description>
            https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.namenode.http-bind-host
            The actual address the HTTP server will bind to. 
            If this optional address is set, it overrides only the hostname portion of dfs.namenode.http-address. 
            It can also be specified per name node or name service for HA/Federation. 
            This is useful for making the name node HTTP server listen on all interfaces by setting it to 0.0.0.0.
        </description>
    </property>

    <property>
        <name>dfs.namenode.https-bind-host</name>
        <value>$HDFS_CONF_dfs_namenode_https___bind___host</value>
        <description>
            https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.namenode.https-bind-host
            The actual address the HTTPS server will bind to. 
            If this optional address is set, it overrides only the hostname portion of dfs.namenode.https-address. 
            It can also be specified per name node or name service for HA/Federation. 
            This is useful for making the name node HTTPS server listen on all interfaces by setting it to 0.0.0.0.
        </description>
    </property>

    <property>
        <name>dfs.client.use.datanode.hostname</name>
        <value>$HDFS_CONF_dfs_client_use_datanode_hostname</value>
        <description>
            https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.client.use.datanode.hostname
            Whether clients should use datanode hostnames when connecting to datanodes.
        </description>
    </property>

    <property>
        <name>dfs.datanode.use.datanode.hostname</name>
        <value>$HDFS_CONF_dfs_datanode_use_datanode_hostname</value>
        <description>
            https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.datanode.use.datanode.hostname
            Whether datanodes should use datanode hostnames when connecting to other datanodes for data transfer.
        </description>
    </property>

    <property>
        <name>dfs.namenode.name.dir</name>
        <value>$HDFS_CONF_dfs_namenode_name_dir</value>
        <description>
            https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.namenode.name.dir
            Determines where on the local filesystem the DFS name node should store the name table(fsimage). 
            If this is a comma-delimited list of directories then the name table is replicated in all of the directories, for redundancy.
        </description>
    </property>

    <property>
        <name>dfs.datanode.data.dir</name>
        <value>$HDFS_CONF_dfs_datanode_data_dir</value>
        <description>
            https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.datanode.data.dir
            Determines where on the local filesystem an DFS data node should store its blocks. 
            If this is a comma-delimited list of directories, then data will be stored in all named directories, typically on different devices. 
            The directories should be tagged with corresponding storage types ([SSD]/[DISK]/[ARCHIVE]/[RAM_DISK]) for HDFS storage policies. 
            The default storage type will be DISK if the directory does not have a storage type tagged explicitly. 
            Directories that do not exist will be created if local filesystem permission allows.
        </description>
    </property>

    <property>
        <name>dfs.namenode.safemode.extension</name>
        <value>$HDFS_CONF_dfs_namenode_safemode_extension</value>
        <description>
            https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml#dfs.namenode.safemode.extension
            Determines extension of safe mode in milliseconds after the threshold level is reached. 
            Support multiple time unit suffix (case insensitive), as described in dfs.heartbeat.interval.
        </description>
    </property>

</configuration>
